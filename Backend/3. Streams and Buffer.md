## Streams 

**"Streams in Node.js are like water flowing through a pipe â€” instead of loading an entire file or data chunk into memory at once, data is processed piece by piece, as it arrives."**
There are **four types** of streams:
- **Readable** (e.g., reading a file)
- **Writable** (e.g., writing to a file)
- **Duplex** (both readable and writable, like a socket)
- **Transform** (like a duplex stream that modifies the data, such as compression)

## Buffer

"A Buffer in Node.js is a temporary storage area for raw binary data â€” basically a chunk of memory that allows us to handle data that's not in a string or object format, like files, streams, or network packets."

"Buffers are especially important in Node because JavaScript doesn't natively handle binary data. Node introduced the Buffer class to efficiently process binary streams â€” for example, when reading a file, receiving data from a socket, or working with images."

"Streams and Buffers are closely related because streams use buffers under the hood to handle data in chunks."

"When data flows through a stream â€” like reading a file or receiving data from a network â€” it doesnâ€™t come all at once. It comes in small pieces, and each of those pieces is stored temporarily in a Buffer before being processed or passed along."

### Read Stream Code

```javascript
const fs = require("fs");
const path = require("path");
const readStream = fs.createReadStream(__dirname + "/test.txt");

readStream.on("data", (chunk) => {
  console.log(chunk);
  console.log(chunk.toString());
});

readStream.on("end", () => {
  console.log("read completed");
});

```

### Write Stream Code

```javascript
const fs = require("fs");
const path = require("path");

const writeStream = fs.createWriteStream(__dirname + "/output.txt", {
  flags: "a",
});

writeStream.write("hello vedant", () => {
  console.log("write complete");
});

```

###  Read Write stream pipe
```javascript
const fs = require("fs");
const path = require("path");
const readStream = fs.createReadStream(__dirname + "/test.txt");

readStream.on("data", (chunk) => {
  console.log(chunk);
  console.log(chunk.toString());
});

readStream.on("end", () => {
  console.log("read completed");
});
```

### Transform Stream
```javascript
const fs = require("fs");
const { Transform } = require("stream");
const path = require("path");
const readStream = fs.createReadStream(__dirname + "/test.txt");

const writeStream = fs.createWriteStream(__dirname + "/output.txt", {
  flags: "w",
});

const UpperCaseTransform = new Transform({
  transform(chunk, encoding, cb) {
    cb(null, chunk.toString().toUpperCase());
  },
});

readStream.pipe(UpperCaseTransform).pipe(writeStream);

writeStream.on("finish", () => {
  console.log("finished writing");
});

```

## Duplex Stream L
```javascript
const { Duplex } = require("stream");

const echoDuplex = new Duplex({
  write(chunk, encoding, callback) {
    console.log(`ðŸ”¹ Received: ${chunk.toString()}`);
    // Echo the data back out
    this.push(`Echo: ${chunk.toString()}`);
    callback();
  },
  read(size) {
    // Nothing to pull on demand here (we only push from write)
  },
});

echoDuplex.on("data", (chunk) => {
  console.log(`ðŸ”¸ Read from stream: ${chunk.toString()}`);
});

echoDuplex.write("Hello Duplex!\n");
echoDuplex.write("Node.js streams rock!\n");
echoDuplex.end();

```